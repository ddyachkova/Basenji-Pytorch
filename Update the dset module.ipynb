{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfad37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, ConcatDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp.grad_scaler import GradScaler\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import r2_score, precision_score, f1_score\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "from itertools import groupby\n",
    "import gzip \n",
    "from io import BytesIO\n",
    "from time import time \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyBigWig\n",
    "from scipy.sparse import csc_matrix\n",
    "import math \n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26a78c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_memmap_data_dir = '/data/users/goodarzilab/darya/work/Datasets/basenji_tutorial_targets_memmaps'\n",
    "targets_memmap_data_dir_cl = '/data/users/goodarzilab/darya/work/Datasets/hg38_targets_memmaps_CL.ATAC'\n",
    "targets_memmap_data_dir_pdx = '/data/users/goodarzilab/darya/work/Datasets/hg38_targets_memmaps_PDX.ATAC'\n",
    "memmap_data_contigs_dir = '/data/users/goodarzilab/darya/work/Datasets/hg19_memmaps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "583c5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files_dir = memmap_data_contigs_dir\n",
    "target_files_dir = [targets_memmap_data_dir_cl, targets_memmap_data_dir_pdx]\n",
    "# target_files_dir = targets_memmap_data_dir\n",
    "\n",
    "mode = 'regression'\n",
    "cut = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1df67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a6e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroms_list = [file.split('_')[0] for file in os.listdir(input_files_dir) if file.split('.')[-1] == 'dta']\n",
    "# shuffle the files\n",
    "np.random.shuffle(chroms_list)\n",
    "\n",
    "input_list = np.hstack([[file for file in os.listdir(input_files_dir) if file.split('_')[0] == chrom] for chrom in chroms_list])\n",
    "\n",
    "\n",
    "if not isinstance(target_files_dir, list): \n",
    "    target_files_dir = [target_files_dir]\n",
    "\n",
    "targets_list = np.array([np.hstack([[file for file in os.listdir(target_dir) if file.split('_')[0] == chrom] for chrom in chroms_list]) for target_dir in target_files_dir])\n",
    "val_input_files = [os.path.join(input_files_dir, file) for file in input_list[int(len(input_list)*cut):]]\n",
    "train_input_files = [os.path.join(input_files_dir, file) for file in input_list[:int(len(input_list)*cut)]]\n",
    "\n",
    "val_target_files = np.array([[os.path.join(target_files_dir[i], target_file) for target_file in targets_list[i][int(targets_list.shape[-1]*cut):]] for i in range(targets_list.shape[0])]).T\n",
    "train_target_files = np.array([[os.path.join(target_files_dir[i], target_file) for target_file in targets_list[i][:int(targets_list.shape[-1]*cut)]] for i in range(targets_list.shape[0])]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ee6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_memmap_input(mmap_name):\n",
    "    '''\n",
    "    Loads a memory map input\n",
    "    '''\n",
    "    seq = np.memmap(mmap_name, dtype='float32',  mode = 'r+') #, shape=(2, self.chrom_seq[self.chrom]))\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cf84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgts = [read_memmap_input(mmap_name) for mmap_name in train_target_files[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c499d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = read_memmap_input(train_input_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90772419",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_targets_lst = [2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a426c90",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'max'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1630633/1074644416.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_targets_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'max'"
     ]
    }
   ],
   "source": [
    "num_targets_lst.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69e1fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_means(tgt, ids):\n",
    "    val = torch.mean(torch.tensor(np.nan_to_num(np.take(tgt, ids))), dim=1)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9afca701",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_len = 3\n",
    "chrom_len = seq.shape[0]\n",
    "target_window = 128\n",
    "idx = 0\n",
    "\n",
    "\n",
    "arr = np.arange(idx*target_window, (idx+1) * target_window)\n",
    "tgt_lst = np.arange(0, chrom_len*max_target_len, chrom_len)\n",
    "ids = np.array([np.split(arr, 128) + tgt for tgt in tgt_lst])\n",
    "stacked_means = torch.cat([get_means(tgts[i], ids[:num_targets_lst[i]]) for i in range(len(tgts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81e6eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [get_means(tgts[i], ids[:num_targets_lst[i]]) for i in range(len(tgts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6bffd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.0045]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44bc539d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0045],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flip(vals[0], dims=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked_means = torch.mean(torch.tensor(np.nan_to_num(np.take(tgt_mmap_cl, ids_cl))), dim=1)\n",
    "# stacked_means_pdx = torch.mean(torch.tensor(np.nan_to_num(np.take(self.tgt_mmap_pdx, ids_pdx))), dim=1)\n",
    "# tgt_window = torch.cat((stacked_means_cl, stacked_means_pdx), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df470fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0546531",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dset = ConcatDataset([Main_Dataset(train_input_files[i], \n",
    "                                        train_target_files[i], [2, 3],\n",
    "                                        128*16,\n",
    "                                        switch=False) \n",
    "                               for i in range(len(train_input_files))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b18eebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(128*128*8 *4/ (128*16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51c53e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=training_dset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0adc90f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1cdb0e3280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1cdb0e3280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1cdb0e3280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1cdb0e3280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1cdb0e3280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1cdb0e3280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1cdb0e3280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1cdb0e3280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/data/users/goodarzilab/darya/anaconda3/envs/basenji_pytorch/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.597992181777954\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "batch = next(iter(train_loader))\n",
    "print (time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1cca5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if isinstance(target_files_dir, list): \n",
    "#     targets_list = np.array([np.hstack([[file for file in os.listdir(target_dir) if file.split('_')[0] == chrom] for chrom in chroms_list]) for target_dir in target_files_dir])\n",
    "# else: \n",
    "#     targets_list = np.expand_dims(np.hstack([[file for file in os.listdir(target_files_dir) if file.split('_')[0] == chrom] for chrom in chroms_list]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aaa4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a283598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main_Dataset(Dataset):\n",
    "    '''\n",
    "    A Dataset class. \n",
    "    For each chromosome, opens the input and target file, \n",
    "    with each __getitem__ call, returns one-hot encoded input and target averaged out over a given window - default is 128. \n",
    "    Attributes:         \n",
    "        target_window (int) - size of the slice from the input and target arrays. Default is 128. \n",
    "        seq (mmap) - input file\n",
    "        tgts_mmap_cl (lst of mmaps) - a list of memory maps corresponding to each target in the target lst \n",
    "        chrom_len (int) - length of the selected chromosome\n",
    "        nucs (arr) - nucleotides encoded to ints, N -> 5\n",
    "        switch (bool) - if True, the nucleotide sequence gets reversed \n",
    "        switch_func (func) - vectorized function to reverse the nuc sequence \n",
    "        num_targets_lst (int) - a list with the number(s) of targets \n",
    "        \n",
    "\n",
    "'''\n",
    "    def __init__(self, input_name, targets_name, num_targets, target_window, switch=False):\n",
    "\n",
    "        self.target_window = target_window\n",
    "        self.seq = self.read_memmap_input(input_name)\n",
    "        self.tgts = [self.read_memmap_input(mmap_name) \n",
    "                for mmap_name in targets_name]\n",
    "#         self.tgt_mmap_cl = self.read_memmap_input(targets_name_cl)\n",
    "#         self.tgt_mmap_pdx = self.read_memmap_input(targets_name_pdx)\n",
    "        self.chrom_len = self.seq.shape[0]\n",
    "        self.nucs = np.arange(6.)\n",
    "        self.switch = switch \n",
    "        self.switch_func = np.vectorize(lambda x: x + 1 if (x % 2 == 0) else x - 1)\n",
    "#         self.num_targets_cl = 2 #23\n",
    "#         self.num_targets_pdx = 2 #14\n",
    "        if not isinstance(num_targets, list):\n",
    "            num_targets = [num_targets]\n",
    "        self.num_targets_lst = num_targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        # length of the dataset is defined as the ration between the full length of the input and the target window\n",
    "        return int(self.seq.shape[0] / self.target_window)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        # slice the input from the memory map\n",
    "        seq_subset = self.seq[idx*self.target_window:(idx+1)*self.target_window]\n",
    "        # if switch=True, reverse the nuc sequence\n",
    "        if self.switch: \n",
    "            seq_subset = self.switch_func(list(reversed(seq_subset)))\n",
    "        # one-hot encode the input, here compressed row format -> sparse matrix conversion is used\n",
    "        dta = self.get_csc_matrix(seq_subset)\n",
    "        max_target_len = max(num_targets_lst)\n",
    "        arr = np.arange(idx*self.target_window, (idx+1) * self.target_window)\n",
    "        tgt_lst = np.arange(0, self.chrom_len*max_target_len, self.chrom_len)\n",
    "        ids = np.array([np.split(arr, 128) + tgt for tgt in tgt_lst])\n",
    "#         print (self.num_targets_lst, [i for i in range(len(self.tgts))])\n",
    "#         print ([self.num_targets_lst[i] for i in range(len(self.tgts))])\n",
    "        stacked_targets_means = torch.cat([self.get_means(self.tgts[i], ids[:self.num_targets_lst[i]]) for i in range(len(self.tgts))])\n",
    "        return torch.tensor(dta), stacked_targets_means \n",
    "\n",
    "    def read_memmap_input(self, mmap_name):\n",
    "        '''\n",
    "        Loads a memory map input\n",
    "        '''\n",
    "        seq = np.memmap(mmap_name, dtype='float32',  mode = 'r+') #, shape=(2, self.chrom_seq[self.chrom]))\n",
    "        return seq\n",
    "\n",
    "    def get_csc_matrix(self, seq_subset):\n",
    "        '''\n",
    "        Converts a compressed row format data to a sparse matrix\n",
    "        ''' \n",
    "        N, M = len(seq_subset), len(self.nucs)\n",
    "        rows, cols = np.arange(N), seq_subset\n",
    "        data = np.ones(N, dtype=np.uint8)\n",
    "        ynew = csc_matrix((data, (rows, cols)), shape=(N, M), dtype=np.uint8)\n",
    "        return ynew.toarray()[:, :4]\n",
    "    \n",
    "    def get_means(self, tgt, ids):\n",
    "        val = torch.mean(torch.tensor(np.nan_to_num(np.take(tgt, ids))), dim=1)\n",
    "        if self.switch: \n",
    "            vals = torch.flip(vals, dims=[0, 1])\n",
    "#         print (val.shape)\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dsets(self, input_files_dir, target_files_dir, num_targets, mode):\n",
    "    cut = self.param_vals.get('cut', .8)\n",
    "    np.random.seed(42)\n",
    "    chroms_list = [file.split('_')[0] for file in os.listdir(input_files_dir) if file.split('.')[-1] == 'dta']\n",
    "    # shuffle the files\n",
    "    np.random.shuffle(chroms_list)\n",
    "    input_list = np.hstack([[file for file in os.listdir(input_files_dir) if file.split('_')[0] == chrom] for chrom in chroms_list])\n",
    "\n",
    "\n",
    "    if not isinstance(target_files_dir, list): \n",
    "        target_files_dir = [target_files_dir]\n",
    "\n",
    "    targets_list = np.array([np.hstack([[file for file in os.listdir(target_dir) if file.split('_')[0] == chrom] for chrom in chroms_list]) for target_dir in target_files_dir])\n",
    "    val_input_files = [os.path.join(input_files_dir, file) for file in input_list[int(len(input_list)*cut):]]\n",
    "    train_input_files = [os.path.join(input_files_dir, file) for file in input_list[:int(len(input_list)*cut)]]\n",
    "\n",
    "    val_target_files = np.array([[os.path.join(target_files_dir[i], target_file) for target_file in targets_list[i][int(targets_list.shape[-1]*cut):]] for i in range(targets_list.shape[0])]).T\n",
    "    train_target_files = np.array([[os.path.join(target_files_dir[i], target_file) for target_file in targets_list[i][:int(targets_list.shape[-1]*cut)]] for i in range(targets_list.shape[0])]).T\n",
    "    \n",
    "    if self.mode=='classification': \n",
    "        self.valid_dset = ConcatDataset([Toy_Dataset(os.path.join(input_files_dir, val_input_files[i]),\n",
    "                                                         self.param_vals.get('target_window', 128)\n",
    "                                                        ) for i in range(len(val_input_files))])\n",
    "\n",
    "        self.training_dset = ConcatDataset([Toy_Dataset(os.path.join(input_files_dir, train_input_files[i]), \n",
    "                                            self.param_vals.get('target_window', 128),\n",
    "                                                     switch=False) for i in range(len(train_input_files))])\n",
    "\n",
    "        self.training_dset_augm = ConcatDataset([Toy_Dataset(os.path.join(input_files_dir, train_input_files[i]),\n",
    "                                                             self.param_vals.get('target_window', 128),\n",
    "                                                          switch=True) for i in range(len(train_input_files))])\n",
    "    else: \n",
    "\n",
    "        self.valid_dset = ConcatDataset([Main_Dataset(val_input_files[i], \n",
    "                                    val_target_files[i], num_targets,\n",
    "                                    self.param_vals.get('target_window', 128),\n",
    "                                    switch=False) \n",
    "                           for i in range(len(train_input_files))])\n",
    "\n",
    "        self.training_dset = ConcatDataset([Main_Dataset(train_input_files[i], \n",
    "                        train_target_files[i], num_targets,\n",
    "                        self.param_vals.get('target_window', 128),\n",
    "                        switch=False) \n",
    "               for i in range(len(train_input_files))])\n",
    "\n",
    "        self.training_dset_augm = ConcatDataset([Main_Dataset(train_input_files[i], \n",
    "                                    train_target_files[i], num_targets,\n",
    "                                    self.param_vals.get('target_window', 128),\n",
    "                                    switch=False) \n",
    "                           for i in range(len(train_input_files))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def make_dsets(input_files_dir, target_files_dir, target_files_dir, mode):\n",
    "#         '''\n",
    "#         Initializes the datasets\n",
    "#         '''\n",
    "#         cut = 0.8 #self.param_vals.get('cut', .8)\n",
    "#         np.random.seed(42)\n",
    "#         # select the files with the .dta extension\n",
    "#         chroms_list = [file.split('_')[0] for file in os.listdir(input_files_dir) if file.split('.')[-1] == 'dta']\n",
    "#         # shuffle the files\n",
    "#         np.random.shuffle(chroms_list)\n",
    "#         # create the input and target file lists for training and validation datasets \n",
    "#         input_list = np.hstack([[file for file in os.listdir(input_files_dir) if file.split('_')[0] == chrom] for chrom in chroms_list])\n",
    "#         targets_cl_list = np.hstack([[file for file in os.listdir(target_files_dir_cl) if file.split('_')[0] == chrom] for chrom in chroms_list])\n",
    "#         targets_pdx_list = np.hstack([[file for file in os.listdir(target_files_dir_pdx) if file.split('_')[0] == chrom] for chrom in chroms_list])\n",
    "        \n",
    "#         val_input_files = input_list[int(len(input_list)*cut):]\n",
    "#         val_target_files_cl = targets_cl_list[int(len(targets_cl_list)*cut):]\n",
    "#         val_target_files_pdx = targets_pdx_list[int(len(targets_pdx_list)*cut):]\n",
    "\n",
    "        \n",
    "#         train_input_files = input_list[:int(len(input_list)*cut)]\n",
    "#         train_target_cl_files = targets_cl_list[:int(len(targets_cl_list)*cut)]\n",
    "#         train_target_pdx_files = targets_pdx_list[:int(len(targets_pdx_list)*cut)]\n",
    "\n",
    "#         # concatenate the datasets defined for each chromosome \n",
    "#         if self.mode=='classification': \n",
    "#             self.valid_dset = ConcatDataset([Toy_Dataset(os.path.join(input_files_dir, val_input_files[i]),\n",
    "#                                                              self.param_vals.get('target_window', 128)\n",
    "#                                                             ) for i in range(len(val_input_files))])\n",
    "        \n",
    "#             self.training_dset = ConcatDataset([Toy_Dataset(os.path.join(input_files_dir, train_input_files[i]), \n",
    "#                                                 self.param_vals.get('target_window', 128),\n",
    "#                                                          switch=False) for i in range(len(train_input_files))])\n",
    "\n",
    "#             self.training_dset_augm = ConcatDataset([Toy_Dataset(os.path.join(input_files_dir, train_input_files[i]),\n",
    "#                                                                  self.param_vals.get('target_window', 128),\n",
    "#                                                               switch=True) for i in range(len(train_input_files))])\n",
    "#         else: \n",
    "#             self.valid_dset = ConcatDataset([DNA_Iter(os.path.join(input_files_dir, val_input_files[i]), \n",
    "#                                                       os.path.join(target_files_dir_cl, val_target_files_cl[i]), \n",
    "#                                                       os.path.join(target_files_dir_pdx, val_target_files_pdx[i]), \n",
    "#                                                       self.param_vals.get('target_window', 128)\n",
    "#                                                       ) for i in range(len(val_input_files))])\n",
    "\n",
    "#             self.training_dset = ConcatDataset([DNA_Iter(os.path.join(input_files_dir, train_input_files[i]), \n",
    "#                                                          os.path.join(target_files_dir_cl, train_target_cl_files[i]), \n",
    "#                                                          os.path.join(target_files_dir_pdx, train_target_pdx_files[i]), \n",
    "#                                                          self.param_vals.get('target_window', 128),\n",
    "#                                                          switch=False) for i in range(len(train_input_files))])\n",
    "\n",
    "#             self.training_dset_augm = ConcatDataset([DNA_Iter(os.path.join(input_files_dir, train_input_files[i]), \n",
    "#                                                               os.path.join(target_files_dir_cl, train_target_cl_files[i]),  \n",
    "#                                                               os.path.join(target_files_dir_pdx, train_target_pdx_files[i]), \n",
    "#                                                               self.param_vals.get('target_window', 128),\n",
    "#                                                               switch=True) for i in range(len(train_input_files))])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basenji_pytorch",
   "language": "python",
   "name": "basenji_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
